{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bc3de7",
   "metadata": {},
   "source": [
    "Set the raw data directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0821535",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../GSE235063_DEV\"  # Replace with your directory path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c080575",
   "metadata": {},
   "source": [
    "Filter, validate file format and extract the processed raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Define the directory containing the files\n",
    "output_dir = os.path.join(input_dir, \"processed_data_csv\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# File patterns to look for\n",
    "file_patterns = [\"processed_barcodes.tsv\", \"processed_genes.tsv\", \"processed_metadata.tsv\", \"processed_matrix.mtx\"]\n",
    "\n",
    "# Loop through files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Check if the file matches any of the desired patterns\n",
    "    if any(pattern in filename for pattern in file_patterns):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        output_file_path = os.path.join(output_dir, filename.replace(\".tsv\", \".csv\").replace(\".mtx\", \".csv\"))\n",
    "\n",
    "        # Extract and convert gzipped files if necessary\n",
    "        if filename.endswith(\".gz\"):\n",
    "            with gzip.open(file_path, 'rt') as gz_file, open(output_file_path, 'w') as out_file:\n",
    "                shutil.copyfileobj(gz_file, out_file)\n",
    "        else:\n",
    "            # For uncompressed files, copy them to the new location with a .csv extension\n",
    "            shutil.copyfile(file_path, output_file_path)\n",
    "\n",
    "print(f\"Processed data has been extracted and saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364cd74",
   "metadata": {},
   "source": [
    "Process only the processed matrix files (extract data, create dense DataFrames from sparse matrices and save as CVS file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879547a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "from scipy.io import mmread\n",
    "import pandas as pd\n",
    "\n",
    "# Define output directories\n",
    "output_dir = os.path.join(input_dir, \"processed_matrices_csv\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process only the *_processed_matrix.mtx.gz files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\"_processed_matrix.mtx.gz\"):\n",
    "        # Define file paths\n",
    "        gz_file_path = os.path.join(input_dir, filename)\n",
    "        decompressed_path = gz_file_path.replace(\".gz\", \"\")\n",
    "        output_csv_path = os.path.join(output_dir, filename.replace(\".mtx.gz\", \".csv\"))\n",
    "\n",
    "        try:\n",
    "            # Decompress the .gz file\n",
    "            with gzip.open(gz_file_path, 'rt') as gz_file:\n",
    "                with open(decompressed_path, 'w') as decompressed_file:\n",
    "                    decompressed_file.write(gz_file.read())\n",
    "\n",
    "            # Read the sparse matrix\n",
    "            sparse_matrix = mmread(decompressed_path)\n",
    "            # Convert to a dense DataFrame\n",
    "            dense_matrix = pd.DataFrame(sparse_matrix.toarray())\n",
    "            # Save the dense matrix as a CSV\n",
    "            dense_matrix.to_csv(output_csv_path, index=False)\n",
    "            print(f\"Processed matrix saved: {output_csv_path}\")\n",
    "\n",
    "            # Clean up: Remove the temporary decompressed file\n",
    "            os.remove(decompressed_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {gz_file_path}: {e}\")\n",
    "\n",
    "print(f\"All processed matrices have been saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e09d42",
   "metadata": {},
   "source": [
    "Merge processed matrices with gene data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory paths\n",
    "processed_matrices_dir = input_dir + '/processed_matrices_csv'\n",
    "genes_dir = \"../GSE235063_DEV\"  # Directory containing *_processed_genes.tsv files\n",
    "\n",
    "# Iterate through all processed matrix files\n",
    "for matrix_file in os.listdir(processed_matrices_dir):\n",
    "    if matrix_file.endswith(\"_processed_matrix.csv\"):\n",
    "        # Define file paths\n",
    "        matrix_path = os.path.join(processed_matrices_dir, matrix_file)\n",
    "        gene_file_name = matrix_file.replace(\"_processed_matrix.csv\", \"_processed_genes.tsv.gz\")\n",
    "        gene_file_path = os.path.join(genes_dir, gene_file_name)\n",
    "        \n",
    "        # Load the matrix file\n",
    "        matrix_df = pd.read_csv(matrix_path)\n",
    "        \n",
    "        # Load the corresponding gene file\n",
    "        if os.path.exists(gene_file_path):\n",
    "            genes_df = pd.read_csv(gene_file_path, header=None, names=[\"Gene\"])  # Assume no header in TSV\n",
    "        else:\n",
    "            print(f\"Gene file missing for {matrix_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Add the \"Gene\" column to the matrix\n",
    "        matrix_df.insert(0, \"Gene\", genes_df[\"Gene\"])\n",
    "        \n",
    "        # Rename all other column headers to \"AML\"\n",
    "        matrix_df.columns = [\"Gene\"] + [\"AML\"] * (len(matrix_df.columns) - 1)\n",
    "        \n",
    "        # Save the updated matrix back to the same file\n",
    "        matrix_df.to_csv(matrix_path, index=False)\n",
    "        print(f\"Updated matrix file: {matrix_path}\")\n",
    "\n",
    "print(\"All files updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8f1f9",
   "metadata": {},
   "source": [
    "Define, train and test the LSTM to extract feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e246fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "output_dir = \"results/lstm_bdm_analysisV2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Train LSTM and Extract Feature Importance\n",
    "def train_lstm_and_extract_importance(sequences, labels, gene_names, patient):\n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    sequences = scaler.fit_transform(sequences)\n",
    "\n",
    "    # Reshape to (samples, timesteps, features)\n",
    "    sequences = sequences.reshape((sequences.shape[0], sequences.shape[1], 1))\n",
    "\n",
    "    # Generate dummy labels matching samples (one per row/gene)\n",
    "    labels = np.random.randint(0, 3, size=sequences.shape[0])  # Placeholder for actual labels\n",
    "\n",
    "    # 70:15:15 split (train, validation, test)\n",
    "    split_1 = int(0.7 * len(sequences))\n",
    "    split_2 = int(0.85 * len(sequences))\n",
    "    X_train, X_val, X_test = (\n",
    "        sequences[:split_1],\n",
    "        sequences[split_1:split_2],\n",
    "        sequences[split_2:],\n",
    "    )\n",
    "    y_train, y_val, y_test = labels[:split_1], labels[split_1:split_2], labels[split_2:]\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False),\n",
    "        Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save training metrics\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(output_dir, f\"{patient}_training_metrics.csv\"), index=False)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=[\"DX\", \"REL\", \"REM\"])\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    with open(os.path.join(output_dir, f\"{patient}_evaluation_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"Classification Report:\\n{report}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{confusion}\\n\")\n",
    "\n",
    "    # Extract feature importance from LSTM weights\n",
    "    lstm_weights = model.layers[0].get_weights()[0]  # LSTM input weights\n",
    "    feature_importance = np.mean(np.abs(lstm_weights), axis=(0, 1))  # Average importance across gates\n",
    "\n",
    "    # Rank features by importance\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        \"Gene\": gene_names,\n",
    "        \"Importance\": feature_importance\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return feature_ranking\n",
    "\n",
    "# Process one patient at a time\n",
    "def process_patient(patient, files):\n",
    "    print(f\"Processing {patient}...\")\n",
    "\n",
    "    # Load DX, REL, REM data\n",
    "    dx_file = os.path.join(processed_matrices_dir, files.get(\"DX\"))\n",
    "    rel_file = os.path.join(processed_matrices_dir, files.get(\"REL\"))\n",
    "    rem_file = os.path.join(processed_matrices_dir, files.get(\"REM\"))\n",
    "\n",
    "    dx_df = pd.read_csv(dx_file)\n",
    "    rel_df = pd.read_csv(rel_file)\n",
    "    rem_df = pd.read_csv(rem_file)\n",
    "\n",
    "    # Extract genes and samples\n",
    "    common_genes = set(dx_df[\"Gene\"]).intersection(rel_df[\"Gene\"]).intersection(rem_df[\"Gene\"])\n",
    "    dx_df = dx_df[dx_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rel_df = rel_df[rel_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rem_df = rem_df[rem_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "\n",
    "    # Combine DX, REL, REM into sequences (samples x timepoints)\n",
    "    sequences = np.stack([dx_df.mean(axis=1).values, rel_df.mean(axis=1).values, rem_df.mean(axis=1).values], axis=1)\n",
    "    labels = np.array([0, 1, 2])  # DX=0, REL=1, REM=2\n",
    "\n",
    "    # Train LSTM and extract feature importance\n",
    "    feature_ranking = train_lstm_and_extract_importance(sequences, labels, dx_df.index.tolist(), patient)\n",
    "\n",
    "    # Save top 100 features\n",
    "    top_100_features = feature_ranking.head(100)\n",
    "    top_100_features.to_csv(os.path.join(output_dir, f\"{patient}_top_100_features.csv\"), index=False)\n",
    "\n",
    "    print(f\"Completed {patient}\")\n",
    "\n",
    "# Organize files by patient\n",
    "file_groups = {}\n",
    "for file in os.listdir(processed_matrices_dir):\n",
    "    if not file.endswith(\"_processed_matrix.csv\"):\n",
    "        continue\n",
    "    parts = file.split(\"_\")\n",
    "    patient, state = parts[1], parts[2]\n",
    "    file_groups.setdefault(patient, {})[state] = file\n",
    "\n",
    "# Run the analysis\n",
    "for patient, files in file_groups.items():\n",
    "    process_patient(patient, files)\n",
    "\n",
    "print(\"LSTM feature importance analysis completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae0c3c",
   "metadata": {},
   "source": [
    "Define, train and test a Bidirectional LSTM to extract feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "output_dir = \"results/bidirectional_lstm_analysis\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Train Bidirectional LSTM and Extract Feature Importance\n",
    "def train_bilstm_and_extract_importance(sequences, labels, gene_names, patient):\n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    sequences = scaler.fit_transform(sequences)\n",
    "\n",
    "    # Reshape to (samples, timesteps, features)\n",
    "    sequences = sequences.reshape((sequences.shape[0], sequences.shape[1], 1))\n",
    "\n",
    "    # Generate dummy labels matching samples (one per row/gene)\n",
    "    labels = np.random.randint(0, 3, size=sequences.shape[0])  # Placeholder for actual labels\n",
    "\n",
    "    # 70:15:15 split (train, validation, test)\n",
    "    split_1 = int(0.7 * len(sequences))\n",
    "    split_2 = int(0.85 * len(sequences))\n",
    "    X_train, X_val, X_test = (\n",
    "        sequences[:split_1],\n",
    "        sequences[split_1:split_2],\n",
    "        sequences[split_2:],\n",
    "    )\n",
    "    y_train, y_val, y_test = labels[:split_1], labels[split_1:split_2], labels[split_2:]\n",
    "\n",
    "    # Build Bidirectional LSTM model\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(32, return_sequences=False)),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Dense(3, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save training metrics\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(output_dir, f\"{patient}_training_metrics.csv\"), index=False)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=[\"DX\", \"REL\", \"REM\"])\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    with open(os.path.join(output_dir, f\"{patient}_evaluation_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"Classification Report:\\n{report}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{confusion}\\n\")\n",
    "\n",
    "    # Extract feature importance from LSTM weights\n",
    "    lstm_weights = model.layers[0].get_weights()[0]  # Input weights of the first LSTM layer\n",
    "    feature_importance = np.mean(np.abs(lstm_weights), axis=(0, 1))  # Average importance across gates\n",
    "\n",
    "    # Rank features by importance\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        \"Gene\": gene_names,\n",
    "        \"Importance\": feature_importance\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return feature_ranking\n",
    "\n",
    "# Process one patient at a time\n",
    "def process_patient(patient, files):\n",
    "    print(f\"Processing {patient}...\")\n",
    "\n",
    "    # Load DX, REL, REM data\n",
    "    dx_file = os.path.join(processed_matrices_dir, files.get(\"DX\"))\n",
    "    rel_file = os.path.join(processed_matrices_dir, files.get(\"REL\"))\n",
    "    rem_file = os.path.join(processed_matrices_dir, files.get(\"REM\"))\n",
    "\n",
    "    dx_df = pd.read_csv(dx_file)\n",
    "    rel_df = pd.read_csv(rel_file)\n",
    "    rem_df = pd.read_csv(rem_file)\n",
    "\n",
    "    # Extract genes and samples\n",
    "    common_genes = set(dx_df[\"Gene\"]).intersection(rel_df[\"Gene\"]).intersection(rem_df[\"Gene\"])\n",
    "    dx_df = dx_df[dx_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rel_df = rel_df[rel_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rem_df = rem_df[rem_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "\n",
    "    # Combine DX, REL, REM into sequences (samples x timepoints)\n",
    "    sequences = np.stack([dx_df.mean(axis=1).values, rel_df.mean(axis=1).values, rem_df.mean(axis=1).values], axis=1)\n",
    "    labels = np.array([0, 1, 2])  # DX=0, REL=1, REM=2\n",
    "\n",
    "    # Train Bidirectional LSTM and extract feature importance\n",
    "    feature_ranking = train_bilstm_and_extract_importance(sequences, labels, dx_df.index.tolist(), patient)\n",
    "\n",
    "    # Save top 100 features\n",
    "    top_100_features = feature_ranking.head(100)\n",
    "    top_100_features.to_csv(os.path.join(output_dir, f\"{patient}_top_100_features.csv\"), index=False)\n",
    "\n",
    "    print(f\"Completed {patient}\")\n",
    "\n",
    "# Organize files by patient\n",
    "file_groups = {}\n",
    "for file in os.listdir(processed_matrices_dir):\n",
    "    if not file.endswith(\"_processed_matrix.csv\"):\n",
    "        continue\n",
    "    parts = file.split(\"_\")\n",
    "    patient, state = parts[1], parts[2]\n",
    "    file_groups.setdefault(patient, {})[state] = file\n",
    "\n",
    "# Run the analysis\n",
    "for patient, files in file_groups.items():\n",
    "    process_patient(patient, files)\n",
    "\n",
    "print(\"Bidirectional LSTM feature importance analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e9830",
   "metadata": {},
   "source": [
    "Define, train and test a Transformer for feature importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, GlobalAveragePooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "output_dir = \"results/transformer_analysis\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Transformer Model Definition\n",
    "def build_transformer_model(input_shape, num_heads, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Multi-Head Attention Layer\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=input_shape[-1])\n",
    "    attention_output = attention_layer(inputs, inputs)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
    "\n",
    "    # Feedforward Network\n",
    "    ffn_output = Dense(64, activation=\"relu\")(attention_output)\n",
    "    ffn_output = Dense(input_shape[-1], activation=\"linear\")(ffn_output)\n",
    "    ffn_output = LayerNormalization(epsilon=1e-6)(ffn_output + attention_output)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    pooled_output = GlobalAveragePooling1D()(ffn_output)\n",
    "\n",
    "    # Final Classification Layer\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(pooled_output)\n",
    "\n",
    "    return Model(inputs, outputs), attention_layer\n",
    "\n",
    "# Train Transformer and Extract Feature Importance\n",
    "def train_transformer_and_extract_importance(sequences, labels, gene_names, patient):\n",
    "    # Normalize data\n",
    "    scaler = MinMaxScaler()\n",
    "    sequences = scaler.fit_transform(sequences)\n",
    "\n",
    "    # Reshape to (samples, timesteps, features)\n",
    "    sequences = sequences.reshape((sequences.shape[0], sequences.shape[1], 1))\n",
    "\n",
    "    # Generate dummy labels matching samples (one per row/gene)\n",
    "    labels = np.random.randint(0, 3, size=sequences.shape[0])  # Placeholder for actual labels\n",
    "\n",
    "    # 70:15:15 split (train, validation, test)\n",
    "    split_1 = int(0.7 * len(sequences))\n",
    "    split_2 = int(0.85 * len(sequences))\n",
    "    X_train, X_val, X_test = (\n",
    "        sequences[:split_1],\n",
    "        sequences[split_1:split_2],\n",
    "        sequences[split_2:],\n",
    "    )\n",
    "    y_train, y_val, y_test = labels[:split_1], labels[split_1:split_2], labels[split_2:]\n",
    "\n",
    "    # Build Transformer model\n",
    "    model, attention_layer = build_transformer_model(\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]), num_heads=4, num_classes=3\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Save training metrics\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(os.path.join(output_dir, f\"{patient}_training_metrics.csv\"), index=False)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=[\"DX\", \"REL\", \"REM\"])\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    with open(os.path.join(output_dir, f\"{patient}_evaluation_metrics.txt\"), \"w\") as f:\n",
    "        f.write(f\"Accuracy: {accuracy}\\n\")\n",
    "        f.write(f\"Classification Report:\\n{report}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{confusion}\\n\")\n",
    "\n",
    "    # Manually extract attention scores using the attention layer\n",
    "    attention_model = Model(inputs=model.input, outputs=attention_layer.call(model.input, model.input))\n",
    "    attention_weights = attention_model.predict(sequences)\n",
    "\n",
    "    # Aggregate attention scores across all heads and timesteps\n",
    "    feature_importance = np.mean(np.abs(attention_weights), axis=(1, 2))  # Average across time and heads\n",
    "\n",
    "    # Rank features by importance\n",
    "    feature_ranking = pd.DataFrame({\n",
    "        \"Gene\": gene_names,\n",
    "        \"Importance\": feature_importance\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return feature_ranking\n",
    "\n",
    "# Process one patient at a time\n",
    "def process_patient(patient, files):\n",
    "    print(f\"Processing {patient}...\")\n",
    "\n",
    "    # Load DX, REL, REM data\n",
    "    dx_file = os.path.join(processed_matrices_dir, files.get(\"DX\"))\n",
    "    rel_file = os.path.join(processed_matrices_dir, files.get(\"REL\"))\n",
    "    rem_file = os.path.join(processed_matrices_dir, files.get(\"REM\"))\n",
    "\n",
    "    dx_df = pd.read_csv(dx_file)\n",
    "    rel_df = pd.read_csv(rel_file)\n",
    "    rem_df = pd.read_csv(rem_file)\n",
    "\n",
    "    # Extract genes and samples\n",
    "    common_genes = set(dx_df[\"Gene\"]).intersection(rel_df[\"Gene\"]).intersection(rem_df[\"Gene\"])\n",
    "    dx_df = dx_df[dx_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rel_df = rel_df[rel_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rem_df = rem_df[rem_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "\n",
    "    # Combine DX, REL, REM into sequences (samples x timepoints)\n",
    "    sequences = np.stack([dx_df.mean(axis=1).values, rel_df.mean(axis=1).values, rem_df.mean(axis=1).values], axis=1)\n",
    "    labels = np.array([0, 1, 2])  # DX=0, REL=1, REM=2\n",
    "\n",
    "    # Train Transformer and extract feature importance\n",
    "    feature_ranking = train_transformer_and_extract_importance(sequences, labels, dx_df.index.tolist(), patient)\n",
    "\n",
    "    # Save top 100 features\n",
    "    top_100_features = feature_ranking.head(100)\n",
    "    top_100_features.to_csv(os.path.join(output_dir, f\"{patient}_top_100_features.csv\"), index=False)\n",
    "\n",
    "    print(f\"Completed {patient}\")\n",
    "\n",
    "# Organize files by patient\n",
    "file_groups = {}\n",
    "for file in os.listdir(processed_matrices_dir):\n",
    "    if not file.endswith(\"_processed_matrix.csv\"):\n",
    "        continue\n",
    "    parts = file.split(\"_\")\n",
    "    patient, state = parts[1], parts[2]\n",
    "    file_groups.setdefault(patient, {})[state] = file\n",
    "\n",
    "# Run the analysis\n",
    "for patient, files in file_groups.items():\n",
    "    process_patient(patient, files)\n",
    "\n",
    "print(\"Transformer feature importance analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ebe4d0",
   "metadata": {},
   "source": [
    "Do DEG extraction and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from pybdm import BDM\n",
    "\n",
    "# Initialize BDM object\n",
    "bdm = BDM(ndim=2)\n",
    "\n",
    "# Define directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "output_dir = \"results/statistical_deg_bdm\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Helper: Compute Differential Expression\n",
    "def compute_deg(dx_df, rel_df, rem_df, logfc_threshold=1.0, pval_threshold=0.05):\n",
    "    epsilon = 1e-9  # Small constant to avoid log(0)\n",
    "    genes = dx_df.index\n",
    "    deg_results = []\n",
    "\n",
    "    for gene in genes:\n",
    "        dx_vals = dx_df.loc[gene].values\n",
    "        rel_vals = rel_df.loc[gene].values\n",
    "        rem_vals = rem_df.loc[gene].values\n",
    "\n",
    "        # Compute log fold changes\n",
    "        logfc_dx_rel = np.log2(np.mean(rel_vals) + epsilon) - np.log2(np.mean(dx_vals) + epsilon)\n",
    "        logfc_dx_rem = np.log2(np.mean(rem_vals) + epsilon) - np.log2(np.mean(dx_vals) + epsilon)\n",
    "        logfc_rel_rem = np.log2(np.mean(rem_vals) + epsilon) - np.log2(np.mean(rel_vals) + epsilon)\n",
    "\n",
    "        # Perform t-tests\n",
    "        pval_dx_rel = ttest_ind(dx_vals, rel_vals, equal_var=False).pvalue\n",
    "        pval_dx_rem = ttest_ind(dx_vals, rem_vals, equal_var=False).pvalue\n",
    "        pval_rel_rem = ttest_ind(rel_vals, rem_vals, equal_var=False).pvalue\n",
    "\n",
    "        deg_results.append({\n",
    "            \"Gene\": gene,\n",
    "            \"LogFC_DX_REL\": logfc_dx_rel,\n",
    "            \"LogFC_DX_REM\": logfc_dx_rem,\n",
    "            \"LogFC_REL_REM\": logfc_rel_rem,\n",
    "            \"Pval_DX_REL\": pval_dx_rel,\n",
    "            \"Pval_DX_REM\": pval_dx_rem,\n",
    "            \"Pval_REL_REM\": pval_rel_rem\n",
    "        })\n",
    "\n",
    "    # Create DEG DataFrame\n",
    "    deg_df = pd.DataFrame(deg_results)\n",
    "\n",
    "    # Adjust p-values using Benjamini-Hochberg correction\n",
    "    deg_df[\"Adj_Pval_DX_REL\"] = multipletests(deg_df[\"Pval_DX_REL\"], method=\"fdr_bh\")[1]\n",
    "    deg_df[\"Adj_Pval_DX_REM\"] = multipletests(deg_df[\"Pval_DX_REM\"], method=\"fdr_bh\")[1]\n",
    "    deg_df[\"Adj_Pval_REL_REM\"] = multipletests(deg_df[\"Pval_REL_REM\"], method=\"fdr_bh\")[1]\n",
    "\n",
    "    # Filter significant DEGs\n",
    "    deg_df = deg_df[\n",
    "        (deg_df[\"Adj_Pval_DX_REL\"] < pval_threshold) |\n",
    "        (deg_df[\"Adj_Pval_DX_REM\"] < pval_threshold) | \n",
    "        (deg_df[\"Adj_Pval_REL_REM\"] < pval_threshold)\n",
    "    ]\n",
    "\n",
    "    # Select top 100 DEGs by absolute logFC\n",
    "    deg_df[\"Abs_LogFC\"] = deg_df[[\"LogFC_DX_REL\", \"LogFC_DX_REM\", \"LogFC_REL_REM\"]].abs().max(axis=1)\n",
    "    top_100_degs = deg_df.nlargest(100, \"Abs_LogFC\")\n",
    "\n",
    "    # Add up/downregulated classification\n",
    "    top_100_degs[\"Regulation\"] = top_100_degs.apply(\n",
    "        lambda row: \"Upregulated\" if row[\"LogFC_DX_REL\"] > 0 else \"Downregulated\", axis=1\n",
    "    )\n",
    "\n",
    "    return top_100_degs\n",
    "\n",
    "# Updated Helper: Compute BDM Perturbation Analysis\n",
    "def compute_bdm(deg_df, dx_df, rel_df, rem_df):\n",
    "    # Align columns (samples) across all DataFrames\n",
    "    common_samples = dx_df.columns.intersection(rel_df.columns).intersection(rem_df.columns)\n",
    "    dx_df = dx_df[common_samples]\n",
    "    rel_df = rel_df[common_samples]\n",
    "    rem_df = rem_df[common_samples]\n",
    "    \n",
    "    bdm_results = []\n",
    "    for gene in deg_df[\"Gene\"]:\n",
    "        dx_vals = dx_df.loc[gene].values\n",
    "        rel_vals = rel_df.loc[gene].values\n",
    "        rem_vals = rem_df.loc[gene].values\n",
    "\n",
    "        # Ensure dimensions match\n",
    "        if len(dx_vals) != len(rel_vals) or len(dx_vals) != len(rem_vals):\n",
    "            print(f\"Skipping gene {gene} due to mismatched dimensions.\")\n",
    "            continue\n",
    "\n",
    "        # Compute binary matrices\n",
    "        binary_dx = (dx_vals > np.median(dx_vals)).astype(int)\n",
    "        binary_rel = (rel_vals > np.median(rel_vals)).astype(int)\n",
    "        binary_rem = (rem_vals > np.median(rem_vals)).astype(int)\n",
    "\n",
    "        # Compute BDM for state transitions\n",
    "        try:\n",
    "            bdm_dx_rel = bdm.bdm(np.vstack([binary_dx, binary_rel]))\n",
    "            bdm_rel_rem = bdm.bdm(np.vstack([binary_rel, binary_rem]))\n",
    "            bdm_dx_rem = bdm.bdm(np.vstack([binary_dx, binary_rem]))\n",
    "\n",
    "            bdm_results.append({\n",
    "                \"Gene\": gene,\n",
    "                \"BDM_DX_REL\": bdm_dx_rel,\n",
    "                \"BDM_REL_REM\": bdm_rel_rem,\n",
    "                \"BDM_DX_REM\": bdm_dx_rem\n",
    "            })\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing gene {gene}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(bdm_results)\n",
    "\n",
    "# Rest of the workflow remains the same.\n",
    "# Main Workflow\n",
    "def process_patient(patient, files):\n",
    "    print(f\"Processing {patient}...\")\n",
    "\n",
    "    dx_file = os.path.join(processed_matrices_dir, files[\"DX\"])\n",
    "    rel_file = os.path.join(processed_matrices_dir, files[\"REL\"])\n",
    "    rem_file = os.path.join(processed_matrices_dir, files[\"REM\"])\n",
    "\n",
    "    # Load the data\n",
    "    dx_df = pd.read_csv(dx_file).set_index(\"Gene\")\n",
    "    rel_df = pd.read_csv(rel_file).set_index(\"Gene\")\n",
    "    rem_df = pd.read_csv(rem_file).set_index(\"Gene\")\n",
    "\n",
    "    # Filter out all-zero genes\n",
    "    dx_df = dx_df[(dx_df.T != 0).any()]\n",
    "    rel_df = rel_df[(rel_df.T != 0).any()]\n",
    "    rem_df = rem_df[(rem_df.T != 0).any()]\n",
    "\n",
    "    # Align indices of all DataFrames (genes)\n",
    "    common_genes = dx_df.index.intersection(rel_df.index).intersection(rem_df.index)\n",
    "    dx_df = dx_df.loc[common_genes]\n",
    "    rel_df = rel_df.loc[common_genes]\n",
    "    rem_df = rem_df.loc[common_genes]\n",
    "\n",
    "    # Compute DEGs\n",
    "    top_100_degs = compute_deg(dx_df, rel_df, rem_df)\n",
    "    top_100_degs.to_csv(os.path.join(output_dir, f\"{patient}_top_100_degs.csv\"), index=False)\n",
    "\n",
    "    # Compute BDM values\n",
    "    bdm_results = compute_bdm(top_100_degs, dx_df, rel_df, rem_df)\n",
    "    bdm_results.to_csv(os.path.join(output_dir, f\"{patient}_bdm_results.csv\"), index=False)\n",
    "\n",
    "# Organize files by patient\n",
    "file_groups = {}\n",
    "for file in os.listdir(processed_matrices_dir):\n",
    "    if not file.endswith(\"_processed_matrix.csv\"):\n",
    "        continue\n",
    "    parts = file.split(\"_\")\n",
    "    patient, state = parts[1], parts[2]\n",
    "    file_groups.setdefault(patient, {})[state] = file\n",
    "\n",
    "# Run the analysis\n",
    "for patient, files in file_groups.items():\n",
    "    process_patient(patient, files)\n",
    "\n",
    "print(\"Statistical DEG and BDM analysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effae54c",
   "metadata": {},
   "source": [
    "Run BDM analysis on the identified DEGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pybdm import BDM\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Initialize BDM object\n",
    "bdm = BDM(ndim=2)\n",
    "\n",
    "# Define directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "top_degs_dir = \"results/statistical_deg_bdm\"\n",
    "bdm_results_dir = \"results/bdm_analysis\"\n",
    "os.makedirs(bdm_results_dir, exist_ok=True)\n",
    "\n",
    "# Thresholds for binarization\n",
    "thresholds = [0.1, 0.5]\n",
    "\n",
    "# Helper: Compute Spearman adjacency matrix\n",
    "def compute_spearman_adjacency(data):\n",
    "    adjacency = np.zeros((data.shape[0], data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(i + 1, data.shape[0]):  # Avoid redundant computation\n",
    "            corr, _ = spearmanr(data[i, :], data[j, :])\n",
    "            adjacency[i, j] = corr\n",
    "            adjacency[j, i] = corr  # Symmetric matrix\n",
    "    return adjacency\n",
    "\n",
    "# Helper: Perform BDM Perturbation Analysis\n",
    "def bdm_analysis(matrix, threshold):\n",
    "    binary_matrix = (matrix > threshold).astype(int)\n",
    "    original_bdm = bdm.bdm(binary_matrix)\n",
    "\n",
    "    # Node perturbation (set each node to 0)\n",
    "    perturbed_bdm_values = []\n",
    "    for i in range(binary_matrix.shape[0]):\n",
    "        perturbed_matrix = binary_matrix.copy()\n",
    "        perturbed_matrix[i, :] = 0\n",
    "        perturbed_matrix[:, i] = 0\n",
    "        perturbed_bdm_values.append(original_bdm - bdm.bdm(perturbed_matrix))\n",
    "\n",
    "    return original_bdm, perturbed_bdm_values\n",
    "\n",
    "# Process each patient's DEGs and compute BDM\n",
    "for patient_file in os.listdir(top_degs_dir):\n",
    "    if not patient_file.endswith(\"_top_100_degs.csv\"):\n",
    "        continue\n",
    "\n",
    "    patient_name = patient_file.split(\"_top_100_degs.csv\")[0]\n",
    "    print(f\"Processing patient: {patient_name}\")\n",
    "\n",
    "    # Create a directory for the patient\n",
    "    patient_bdm_dir = os.path.join(bdm_results_dir, patient_name)\n",
    "    os.makedirs(patient_bdm_dir, exist_ok=True)\n",
    "\n",
    "    # Load top 100 DEGs\n",
    "    top_degs = pd.read_csv(os.path.join(top_degs_dir, patient_file))[\"Gene\"].values\n",
    "\n",
    "    # Load DX, REL, and REM data\n",
    "    try:\n",
    "        dx_file = next(f for f in os.listdir(processed_matrices_dir) if f\"{patient_name}_DX\" in f)\n",
    "        rel_file = next(f for f in os.listdir(processed_matrices_dir) if f\"{patient_name}_REL\" in f)\n",
    "        rem_file = next(f for f in os.listdir(processed_matrices_dir) if f\"{patient_name}_REM\" in f)\n",
    "    except StopIteration:\n",
    "        print(f\"Missing files for patient {patient_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    dx_df = pd.read_csv(os.path.join(processed_matrices_dir, dx_file)).set_index(\"Gene\").loc[top_degs]\n",
    "    rel_df = pd.read_csv(os.path.join(processed_matrices_dir, rel_file)).set_index(\"Gene\").loc[top_degs]\n",
    "    rem_df = pd.read_csv(os.path.join(processed_matrices_dir, rem_file)).set_index(\"Gene\").loc[top_degs]\n",
    "\n",
    "    # Concatenate counts\n",
    "    combined_matrix = np.hstack([dx_df.values, rel_df.values, rem_df.values])\n",
    "\n",
    "    # Compute adjacency matrix\n",
    "    adjacency_matrix = compute_spearman_adjacency(combined_matrix)\n",
    "\n",
    "    # Perform BDM analysis for each threshold\n",
    "    for threshold in thresholds:\n",
    "        original_bdm, perturbed_bdm_values = bdm_analysis(adjacency_matrix, threshold)\n",
    "\n",
    "        # Save results\n",
    "        result_path = os.path.join(patient_bdm_dir, f\"{patient_name}_bdm_threshold_{threshold}.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"Gene\": top_degs,\n",
    "            \"Perturbed_BDM\": perturbed_bdm_values\n",
    "        }).to_csv(result_path, index=False)\n",
    "\n",
    "print(\"BDM analysis completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b49f3",
   "metadata": {},
   "source": [
    "Run NNMF clustering to decompose the gene expression matrices into transcriptional co-expression modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.stats import spearmanr\n",
    "from pybdm import BDM\n",
    "\n",
    "# Initialize BDM object\n",
    "bdm = BDM(ndim=2)\n",
    "\n",
    "# Directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "nnmf_results_dir = \"results/nnmf_analysis\"\n",
    "os.makedirs(nnmf_results_dir, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "n_components = 5  # Number of transcriptional modules\n",
    "top_genes_per_module = 50  # Top genes to keep per module for BDM analysis\n",
    "thresholds = [0.1, 0.5]  # Thresholds for binarization\n",
    "\n",
    "# Helper: Compute Spearman adjacency matrix\n",
    "def compute_spearman_adjacency(data):\n",
    "    adjacency = np.zeros((data.shape[0], data.shape[0]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(i + 1, data.shape[0]):  # Avoid redundant computation\n",
    "            corr, _ = spearmanr(data[i, :], data[j, :])\n",
    "            adjacency[i, j] = corr\n",
    "            adjacency[j, i] = corr  # Symmetric matrix\n",
    "    return adjacency\n",
    "\n",
    "# Helper: Perform BDM Perturbation Analysis\n",
    "def bdm_analysis(matrix, threshold):\n",
    "    binary_matrix = (matrix > threshold).astype(int)\n",
    "    original_bdm = bdm.bdm(binary_matrix)\n",
    "\n",
    "    # Node perturbation (set each node to 0)\n",
    "    perturbed_bdm_values = []\n",
    "    for i in range(binary_matrix.shape[0]):\n",
    "        perturbed_matrix = binary_matrix.copy()\n",
    "        perturbed_matrix[i, :] = 0\n",
    "        perturbed_matrix[:, i] = 0\n",
    "        perturbed_bdm_values.append(original_bdm - bdm.bdm(perturbed_matrix))\n",
    "\n",
    "    return perturbed_bdm_values\n",
    "\n",
    "# Helper: Apply NNMF\n",
    "def apply_nnmf(expression_matrix, n_components):\n",
    "    model = NMF(n_components=n_components, init=\"random\", random_state=42, max_iter=1000)\n",
    "    W = model.fit_transform(expression_matrix)\n",
    "    H = model.components_\n",
    "    return W, H\n",
    "\n",
    "# Process each patient's data\n",
    "patients = set([file.split(\"_\")[1] for file in os.listdir(processed_matrices_dir) if file.endswith(\"_processed_matrix.csv\")])\n",
    "\n",
    "for patient_id in patients:\n",
    "    print(f\"Processing patient: {patient_id}\")\n",
    "\n",
    "    # Create directories\n",
    "    patient_nnmf_dir = os.path.join(nnmf_results_dir, patient_id)\n",
    "    patient_bdm_dir = os.path.join(patient_nnmf_dir, \"bdm_results\")\n",
    "    os.makedirs(patient_nnmf_dir, exist_ok=True)\n",
    "    os.makedirs(patient_bdm_dir, exist_ok=True)\n",
    "\n",
    "    # Construct file paths and check existence\n",
    "    try:\n",
    "        dx_file = next(f for f in os.listdir(processed_matrices_dir) if f\"_{patient_id}_DX_processed_matrix.csv\" in f)\n",
    "        rel_file = next(f for f in os.listdir(processed_matrices_dir) if f\"_{patient_id}_REL_processed_matrix.csv\" in f)\n",
    "        rem_file = next(f for f in os.listdir(processed_matrices_dir) if f\"_{patient_id}_REM_processed_matrix.csv\" in f)\n",
    "    except StopIteration:\n",
    "        print(f\"Missing one or more files for patient {patient_id}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    dx_df = pd.read_csv(os.path.join(processed_matrices_dir, dx_file)).set_index(\"Gene\")\n",
    "    rel_df = pd.read_csv(os.path.join(processed_matrices_dir, rel_file)).set_index(\"Gene\")\n",
    "    rem_df = pd.read_csv(os.path.join(processed_matrices_dir, rem_file)).set_index(\"Gene\")\n",
    "\n",
    "    # Concatenate expression data\n",
    "    combined_matrix = pd.concat([dx_df, rel_df, rem_df], axis=1).values\n",
    "    gene_names = dx_df.index.values\n",
    "\n",
    "    # Apply NNMF\n",
    "    W, H = apply_nnmf(combined_matrix, n_components)\n",
    "\n",
    "    # Identify top genes per module\n",
    "    nnmf_results = []\n",
    "    for module_idx in range(W.shape[1]):\n",
    "        module_scores = W[:, module_idx]\n",
    "        ranked_genes = np.argsort(module_scores)[::-1]  # Sort in descending order\n",
    "        top_genes = ranked_genes[:top_genes_per_module]\n",
    "        for gene_idx in top_genes:\n",
    "            nnmf_results.append({\n",
    "                \"Module\": module_idx + 1,\n",
    "                \"Gene\": gene_names[gene_idx],\n",
    "                \"Score\": module_scores[gene_idx]\n",
    "            })\n",
    "\n",
    "    nnmf_results_df = pd.DataFrame(nnmf_results)\n",
    "    nnmf_results_df.to_csv(os.path.join(patient_nnmf_dir, f\"{patient_id}_nnmf_top_genes.csv\"), index=False)\n",
    "\n",
    "    # Get unique top genes across modules for BDM\n",
    "    unique_top_genes = nnmf_results_df[\"Gene\"].unique()\n",
    "    top_gene_matrix = pd.DataFrame(combined_matrix, index=gene_names).loc[unique_top_genes].values\n",
    "\n",
    "    # Compute adjacency matrix for BDM\n",
    "    adjacency_matrix = compute_spearman_adjacency(top_gene_matrix)\n",
    "\n",
    "    # Perform BDM analysis for each threshold\n",
    "    for threshold in thresholds:\n",
    "        perturbed_bdm_values = bdm_analysis(adjacency_matrix, threshold)\n",
    "        result_path = os.path.join(patient_bdm_dir, f\"{patient_id}_bdm_threshold_{threshold}.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"Gene\": unique_top_genes,\n",
    "            \"Perturbed_BDM\": perturbed_bdm_values\n",
    "        }).to_csv(result_path, index=False)\n",
    "\n",
    "print(\"NNMF and BDM analysis completed for all patients.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa068bf",
   "metadata": {},
   "source": [
    "Run Bayesian Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Directories\n",
    "processed_matrices_dir = input_dir + \"/processed_matrices_csv\"\n",
    "ridge_folder = \"Bayesian_regression/Bayesian_Ridge\"\n",
    "os.makedirs(ridge_folder, exist_ok=True)\n",
    "\n",
    "# Helper Function: Load and Process Patient Data\n",
    "def load_patient_data(files):\n",
    "    dx_file = files.get(\"DX\")\n",
    "    rel_file = files.get(\"REL\")\n",
    "    rem_file = files.get(\"REM\")\n",
    "    \n",
    "    if not dx_file or not rel_file or not rem_file:\n",
    "        raise ValueError(\"Missing one or more files (DX, REL, REM) for patient.\")\n",
    "\n",
    "    dx_df = pd.read_csv(os.path.join(processed_matrices_dir, dx_file))\n",
    "    rel_df = pd.read_csv(os.path.join(processed_matrices_dir, rel_file))\n",
    "    rem_df = pd.read_csv(os.path.join(processed_matrices_dir, rem_file))\n",
    "\n",
    "    common_genes = set(dx_df[\"Gene\"]).intersection(rel_df[\"Gene\"]).intersection(rem_df[\"Gene\"])\n",
    "    dx_df = dx_df[dx_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rel_df = rel_df[rel_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "    rem_df = rem_df[rem_df[\"Gene\"].isin(common_genes)].set_index(\"Gene\")\n",
    "\n",
    "    combined_data = pd.concat([dx_df.mean(axis=1), rel_df.mean(axis=1), rem_df.mean(axis=1)], axis=1)\n",
    "    combined_data.columns = [\"DX\", \"REL\", \"REM\"]\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Bayesian Ridge Regression\n",
    "def bayesian_ridge_regression(patient, files):\n",
    "    print(f\"Processing Bayesian Ridge Regression for patient: {patient}...\")\n",
    "    data = load_patient_data(files)\n",
    "\n",
    "    X = data.values.T  # Transpose to treat genes as samples\n",
    "    y = np.arange(X.shape[0])  # Dummy labels for each gene\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    feature_importance = pd.DataFrame({\n",
    "        \"Gene\": data.index,\n",
    "        \"Importance\": np.abs(model.coef_)\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    output_file = os.path.join(ridge_folder, f\"{patient}_ridge_importance.csv\")\n",
    "    feature_importance.head(30).to_csv(output_file, index=False)  # Save top 30 features\n",
    "    print(f\"Saved Bayesian Ridge results for patient: {patient}\")\n",
    "\n",
    "# Organize files by patient\n",
    "file_groups = {}\n",
    "for file in os.listdir(processed_matrices_dir):\n",
    "    if not file.endswith(\"_processed_matrix.csv\"):\n",
    "        continue\n",
    "    parts = file.split(\"_\")\n",
    "    patient, state = parts[1], parts[2]\n",
    "    file_groups.setdefault(patient, {})[state] = file\n",
    "\n",
    "# Run Analysis for All Patients\n",
    "for patient, files in file_groups.items():\n",
    "    try:\n",
    "        bayesian_ridge_regression(patient, files)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing patient {patient}: {e}\")\n",
    "\n",
    "print(\"Bayesian Ridge Regression completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "longamlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
